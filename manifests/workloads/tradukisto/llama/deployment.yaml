apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama
  namespace: tradukisto
  labels:
    app.kubernetes.io/name: llama
    app.kubernetes.io/part-of: tradukisto
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: llama
  template:
    metadata:
      annotations:
        descheduler.alpha.kubernetes.io/prefer-no-eviction: "true"
      labels:
        app.kubernetes.io/name: llama
        app.kubernetes.io/part-of: tradukisto
    spec:
      runtimeClassName: nvidia
      containers:
        - name: llama
          image: ghcr.io/ggml-org/llama.cpp:server-cuda
          ports:
            - containerPort: 8080
          env:
            - name: LLAMA_ARG_HF_REPO
              value: "mmnga/plamo-2-translate-gguf:Q3_K_M"
            - name: LLAMA_ARG_HOST
              value: "0.0.0.0"
            - name: LLAMA_ARG_PORT
              value: "8080"
            - name: LLAMA_ARG_CTX_SIZE
              value: "8192"
            - name: LLAMA_ARG_N_PARALLEL
              value: "2"
            - name: LLAMA_ARG_N_SEQ_MAX
              value: "2"
            - name: LLAMA_ARG_CTX_SEQ
              value: "4096"
            - name: LLAMA_ARG_N_PREDICT
              value: "8192"
            - name: LLAMA_ARG_BATCH
              value: "256"
            - name: LLAMA_ARG_UBATCH
              value: "256"
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
              nvidia.com/gpu: "1"
            limits:
              memory: "1Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: llama-cache
              mountPath: /root/.cache/llama.cpp
        - name: gpu-monitor
          image: nvidia/cuda:12.9.1-runtime-ubuntu22.04
          command: ["/bin/bash", "-lc", "sleep infinity"]
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "100m"
              memory: "128Mi"
      volumes:
        - name: llama-cache
          persistentVolumeClaim:
            claimName: llama-cache
